using Flux, Zygote, Functors

# layer with unique connection for each input and output
struct OneToOne
    weight::AbstractArray   # weight matrix
    bias::AbstractArray  # bias vector
    Ïƒ::Function # activation fn
end
@functor OneToOne (weight,bias)

# m:Int -> f:(Float -> Float) -> OneToOne m f
# OneToOne constructor
function OneToOne(d::Integer, Ïƒ = identity)
    #weight = randn(Float32, d)
    weight = ones(Float32, d)
    bias = zeros(Float32, d)
    return OneToOne(weight, bias, Ïƒ)
end

# âˆ€ n:Int âˆƒ m:Int -> [Float m n] -> [Float m n]
# OneToOne application
function (l::OneToOne)(x)
    return l.Ïƒ(l.weight .* x .+ l.bias)
end

# âˆ€ m,n:Int f:(Float -> Float)  -> g:(OneToOne m f) -> A:[Float m n] -> (g A, âˆ‡ g A)
function Zygote.pullback(l::OneToOne, x::AbstractArray)
    y = l.weight .* x .+ l.bias
    z = l.Ïƒ(y)
    
    function B(Î”)
        Î” = Î” .* gradient(layer.Ïƒ, y)[1]  # chain rule applied
        âˆ‡_w = sum(Î” .* x, dims=2)[:]  # Gradient w.r.t. weight
        âˆ‡_b = sum(Î”, dims=2)[:]       # Gradient w.r.t. bias
        return (weight=âˆ‡_w, bias=âˆ‡_b), nothing
    end
    
    return z, B
end

mutable struct Softmax
    weight::AbstractArray
    bias::AbstractArray
    Ïƒ::Function
end
@functor Softmax (weight,bias)

function Softmax(d::Integer)
    weight = randn(Float32,d)
    bias = randn(Float32,d)
    return Softmax(weight,bias)
end

function (Î¸::Softmax)(x)
    return Î¸.Ïƒ(Î¸.weight .* x .+ Î¸.bias)
end

#function Zygote.pullback(Î¸::Softmax,x::AbstractArray)
#    y = Î¸(x)
#    function back(Î”)
#        Î” = Î” .* gradient(Î¸,y)[1]
#        âˆ‡_w = sum(Î” .* x,dims=2)

mutable struct SoftNN
    Ïƒ::Function
end
#
#function (Î´::SoftNN)(x)
#    m,n = size(x)
#    D = euclidean(x)
#    D = D .* (1 .- I(n))

struct DEWAK
    Î¸_e::Dense
    Ï‰::OneToOne
    Îº::Parallel
    Î¸_d::Dense
end
@functor DEWAK

function (m::DEWAK)(X::AbstractMatrix)
    E = (m.Ï‰ âˆ˜ m.Î¸_e)(X)
    D = 1 ./ (euclidean(E) .+ eps(Float32))
    D = wak(D)
    XÌ‚ = m.Î¸_d((D * E')')
    return XÌ‚
end

struct WeightNetwork
    l::Chain
end
@functor WeightNetwork

# âˆ€ n:Int âˆƒ m:Int -> [Float m n] -> [Float m n]
function (m::WeightNetwork)(X::AbstractMatrix)
    return (softmax âˆ˜ m.l)(X)
end

struct WeightedEncoder
    encoder::Chain
    weigher::WeightNetwork
end
@functor WeightedEncoder

# âˆ€ n:Int âˆƒ m:Int -> [Float m n] -> [Float m n]
function (M::WeightedEncoder)(X::AbstractMatrix)
    E = M.encoder(X)
    w = M.weigher(E)
    return w .* E
end

struct ClustNetwork
    l::Chain
end
@functor ClustNetwork

# âˆ€ n:Int âˆƒ m,c:Int -> [Float m n] -> [Float c n]
function (m::ClustNetwork)(X::AbstractMatrix)
    C = (softmax âˆ˜ m.l)(X)
    return C
end

struct DeePWAK
    encoder::WeightedEncoder
    partitioner::ClustNetwork
    decoder::Chain
    metric::Function
end
@functor DeePWAK (encoder,partitioner,decoder)

# âˆ€ n:Int âˆƒ m:Int -> [[Float]]{m,n} -> [[Float]]{m,n}
function (m::DeePWAK)(X::AbstractMatrix)
    #w = (softmax âˆ˜ m.weigher)(X)
    #E = w .* m.encoder(X)
    E = m.encoder(X)
    D = m.metric(E)
    C = m.partitioner(E)
    P = C' * C
    G = wak(D .* P)
    Ehat = (G * E')'
    return m.decoder(Ehat)
end

#âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> [Float d n]
function getw(M::DeePWAK,X::AbstractMatrix)
    #E = M.encoder(X)
    #w = (softmax âˆ˜ M.weigher)(E)
    return M.encoder.weigher(X)
end

#âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> [Float d n]
function embedding(M::DeePWAK,X::AbstractMatrix)
    E = M.encoder(X)
    w = (softmax âˆ˜ M.weigher)(E)
    return w .* E
end

#âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> [Float n n]
function dist(M::DeePWAK,X::AbstractMatrix)
    E = M.encoder(X)
    #return 1 ./ (euclidean(E) .+ eps(Float32))
    return M.metric(E)
end

#âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> [Float c n]
function clust(M::DeePWAK,X::AbstractMatrix)
    E = embedding(M,X)
    C = (softmax âˆ˜ M.partitioner)(E)
    return C
end

#âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> [Float n]
function getclusts(M::DeePWAK,X::AbstractMatrix)
    return map(x->x[1],argmax(clust(M,X),dims=1))
end


#âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> [Float n n]
function g(M::DeePWAK,X::AbstractMatrix)
    D = dist(M,X)
    C = clust(M,X)
    P = C' * C
    return wak(D .* P)
end

# âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> [Float m n]
function(M::DeePWAK)(X::AbstractMatrix)
    E = embedding(M,X)
    G = g(M,X)
    EÌ‚ = (G * E')'
    XÌ‚ = M.dm(EÌ‚)
    return XÌ‚
end

# âˆ€ m,d,c:Int -> DeePWAK m d c -> Float
function H(m::DeePWAK)
    H_Ï‰ = (mean âˆ˜ H)(m.Ï‰.weight)
    return H_Ï‰
end

# âˆ€ n:Int âˆƒ m,d,c:Int -> DeePWAK m d c -> [Float m n] -> Float -> Float
function softmod(M::DeePWAK,X::AbstractMatrix,Î³)
    D = dist(M,X)
    C = clust(M,X)
    P = C' * C
    return softmod(D,P,Î³)
end

function stats(M::DeePWAK,X::AbstractArray,Î³)
    L = mse(M,X)
    w = (softmax âˆ˜ M.dd âˆ˜ M.dm)(X)
    H_w = (mean âˆ˜ H)(w,dims=2)
    calH = softmod(M,X,Î³)
    return L,H_w,calH
end

function loss(M::DeePWAK,X::AbstractArray,Î³)
    #L = mse(M,X)
    #w = (M.dd âˆ˜ M.dm)(X)
    #H_w = H(w)
    #calH = softmod(M,X,Î³)
    L,H_w,calH = stats(M,X,Î³)
    return L * H_w / calH#L + Î± * H_m - Î² * calH
end

function mse(m::Union{Chain,DeePWAK},X::AbstractMatrix)
    return Flux.mse(X,m(X))
end
    
function stats(M::DeePWAK,X::AbstractArray,Y::AbstractArray,Î³)
    L = mse(M,X,Y)
    w = (softmax âˆ˜ M.dd âˆ˜ M.dm)(X)
    H_w = (mean âˆ˜ H)(w,dims=2)
    calH = softmod(M,X,Î³)
    return L,H_w,calH
end


function loss(M::DeePWAK,X::AbstractArray,Y::AbstractArray,Î³)
    #L = mse(M,X)
    #w = (M.dd âˆ˜ M.dm)(X)
    #H_w = H(w)
    #calH = softmod(M,X,Î³)
    L,H_w,calH = stats(M,X,Y,Î³)
    return L * H_w / calH#L + Î± * H_m - Î² * calH
end

function mse(m::Union{Chain,DeePWAK},X::AbstractMatrix,Y::AbstractMatrix)
    return Flux.mse(Y,m(X))
end
    
function update!(m::Union{Chain,ClustNetwork,WeightNetwork,WeightedEncoder},
                 loss::Function,opt)
    state = Flux.setup(opt,m)
    l,âˆ‡ = Flux.withgradient(loss,m)
    Flux.update!(state,m,âˆ‡[1])
    return l
end

function update!(M::DeePWAK,loss::Function,opt)
    state = Flux.setup(opt,M)
    l,âˆ‡ = Flux.withgradient(loss,M)
    Flux.update!(state,M,âˆ‡[1])
    return l
end

function train!(M::DeePWAK,X::AbstractMatrix,opt,Î³)
    l,entropy,modularity = stats(M,X,Î³)
    update!(m,Î¸->loss(Î¸,X),opt)
    #entropy = update!(m,H,opt)
    #modularity = update!(m,Î¸->1/softmod(Î¸,X,Î³),opt)
    return l, entropy, modularity
end

#function trainencoder!(m::DeePWAK,X::AbstractMatrix,opt)
    
    

function encoderlayers(m::Integer,d::Integer,l::Integer,Ïƒ=relu)
    #dims = size(X)
    #m = prod(dims[1:(length(dims)-1)])
    s = div(d-m,l)
    ğ = collect(m:s:d)
    ğ[l+1] = d
    #ğ = vcat(ğ,reverse(ğ[1:length(ğ)-1]))
    Î¸ = foldl(ğ[3:length(ğ)],
              init=Chain(Dense(ğ[1] => ğ[2],Ïƒ))) do layers,d
        d_0 = size(layers[length(layers)].weight)[1]
        return Chain(layers...,Dense(d_0 => d,Ïƒ))
    end
end
    
